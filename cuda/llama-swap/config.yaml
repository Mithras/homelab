# https://github.com/mostlygeek/llama-swap

healthCheckTimeout: 600
logLevel: info
models:
  # https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF
  qwen3moe_instruct:
    cmd: >
      /app/llama-server
      -hf unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q6_K_XL
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm layer
      -ts 2,1

      --flash-attn on
      --no-webui
      --ctx-size 262144
      --jinja
      --reasoning-format deepseek
      --reasoning-budget 0
      --alias qwen3moe_instruct

      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0
      --presence-penalty 1.0
    # --kv-unified
    ttl: 600
  # https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF
  qwen3moe_thinking:
    cmd: >
      /app/llama-server
      -hf unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q6_K_XL
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm layer
      -ts 2,1

      --flash-attn on
      --no-webui
      --ctx-size 262144
      --jinja
      --reasoning-format deepseek
      --alias qwen3moe_thinking

      --temp 0.6
      --top-p 0.95
      --top-k 20
      --min-p 0
      --presence-penalty 1.0
    ttl: 600
  # https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF
  qwen3moe_coder:
    cmd: >
      /app/llama-server
      -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q6_K_XL
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm layer
      -ts 2,1

      --flash-attn on
      --no-webui
      --ctx-size 262144
      --jinja
      --reasoning-format deepseek
      --alias qwen3moe_coder

      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0
      --repeat-penalty 1.05
    ttl: 600
  # https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF
  seed_oss:
    cmd: >
      /app/llama-server
      -hf unsloth/Seed-OSS-36B-Instruct-GGUF:Q4_K_XL
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm layer
      -ts 2,1

      --flash-attn on
      --no-webui
      --ctx-size 131072
      --jinja
      --reasoning-format auto
      --alias seed_oss
      --chat-template-kwargs '{"thinking_budget": 512}'
    ttl: 600

  # https://huggingface.co/unsloth/GLM-4.5-Air-GGUF
  glm_air:
    cmd: >
      /app/llama-server
      -m /download/GLM-4.5-Air-IQ4_XS/GLM-4.5-Air-IQ4_XS.gguf
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0

      --flash-attn on
      --no-webui
      --ctx-size 131072
      --jinja
      --reasoning-format auto
      --alias glm_air

      -sm layer
      -ts 31,17
      --n-cpu-moe 15
    # IQ4_XS:
    #   -ts 31,17
    #   --n-cpu-moe 15
    #   TPS: 63.19 / 33.45
    # Q4_K_M:
    #   -ts 34,14
    #   --n-cpu-moe 21
    #   TPS: 64.32 / 20.62
    ttl: 600

  # https://huggingface.co/unsloth/gpt-oss-120b-GGUF
  gpt_oss:
    cmd: >
      /app/llama-server
      -m /download/gpt-oss-120b/Q4_K_M/gpt-oss-120b-Q4_K_M.gguf
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0

      --flash-attn on
      --no-webui
      --ctx-size 131072
      --jinja
      --reasoning-format auto
      --alias gpt_oss

      -sm layer
      -ts 100,65
      --n-cpu-moe 6
    # Q4_K_M
    #   -ts 100,65
    #   --n-cpu-moe 6
    #   TPS: 138.43 / 74.09
    ttl: 600

  # https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF
  _qwen3_embedding:
    cmd: >
      /app/llama-server
      -hf Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm none

      --no-webui
      --alias qwen3_embedding

      --embedding
      --pooling last
      -ub 8192
    env:
      - CUDA_VISIBLE_DEVICES=1
  # https://huggingface.co/Mungert/Qwen3-Reranker-0.6B-GGUF
  # https://github.com/ggml-org/llama.cpp/pull/14029
  _qwen3_reranker:
    cmd: >
      /app/llama-server
      -hf Mungert/Qwen3-Reranker-0.6B-GGUF:Q8_0
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm none

      --no-webui
      --alias qwen3_reranker

      --reranking
    env:
      - CUDA_VISIBLE_DEVICES=1
  # https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-GGUF
  _qwen3_task:
    cmd: >
      /app/llama-server
      -hf unsloth/Qwen3-4B-Instruct-2507-GGUF:Q5_K_XL
      -ngl 99
      --threads -1
      --port ${PORT}
      -ctk q8_0
      -ctv q8_0
      -sm none

      --flash-attn on
      --no-webui
      --ctx-size 32684
      --jinja
      --reasoning-format deepseek
      --reasoning-budget 0
      --alias qwen3_task

      --temp 0.7
      --top-p 0.8
      --top-k 20
      --min-p 0
      --presence-penalty 1.0
    env:
      - CUDA_VISIBLE_DEVICES=1

groups:
  chat:
    persistent: false
    swap: true
    exclusive: false
    members:
      - qwen3moe_instruct
      - qwen3moe_thinking
      - qwen3moe_coder
      - seed_oss
      - glm_air
      - gpt_oss
  rag:
    persistent: false
    swap: false
    exclusive: false
    members:
      - qwen3_embedding
      - qwen3_reranker
  task:
    persistent: false
    swap: true
    exclusive: false
    members:
      - qwen3_task

hooks:
  on_startup:
    preload: []
