x-gpu: &gpu
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
services:
  # wyoming-whisper:
  #   <<: [ *gpu ]
  #   # image: rhasspy/wyoming-whisper
  #   build:
  #     args:
  #       - BASE=nvidia/cuda:12.8.0-cudnn-runtime-ubuntu22.04
  #       # https://github.com/rhasspy/wyoming-faster-whisper/releases
  #       - WYOMING_WHISPER_VERSION=2.4.0
  #     context: ./wyoming-whisper
  #   command:
  #     - --device
  #     - cuda
  #     - --model
  #     # - large-v3 # gpu: 0.25s
  #     - deepdml/faster-whisper-large-v3-turbo-ct2 # gpu: 0.2s
  #     - --language
  #     - en
  #     - --debug
  #   container_name: wyoming-whisper
  #   restart: unless-stopped
  #   hostname: wyoming-whisper
  #   volumes:
  #     - ./wyoming-whisper/data:/data
  #   ports:
  #     - 10300:10300
  #   networks:
  #     - homelab
  wyoming-whisper-2:
    <<: [ *gpu ]
    # https://github.com/linuxserver/docker-faster-whisper
    image: lscr.io/linuxserver/faster-whisper:gpu
    container_name: wyoming-whisper-2
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/Los_Angeles
      # - WHISPER_MODEL=distil-large-v3
      - WHISPER_MODEL=large-v3-turbo
      - WHISPER_BEAM=5
      - WHISPER_LANG=en
    restart: unless-stopped
    hostname: wyoming-whisper-2
    volumes:
      - ./wyoming-whisper-2/data:/config
    ports:
      - 10301:10300
    networks:
      - homelab
  ollama:
    # Generation
    # + ollama pull hf.co/bartowski/Replete-LLM-V2.5-Qwen-32b-GGUF:Q4_K_M
    # ? ollama pull hf.co/bartowski/DeepSeek-R1-Distill-Qwen-32B-GGUF:Q4_K_M
    # Embedding
    # + ollama pull mxbai-embed-large
    # Reranking
    # + ollama pull hf.co/gpustack/bge-reranker-v2-m3-GGUF:Q8_0
    <<: [ *gpu ]
    image: ollama/ollama
    container_name: ollama
    restart: unless-stopped
    hostname: ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_KEEP_ALIVE=300
    volumes:
      - ./ollama/data:/root/.ollama
    ports:
      - 11434:11434
    networks:
      - homelab
  # sd-download:
  #   build: https://github.com/AcdbdBarho/stable-diffusion-webui-docker.git#:services/download
  #   container_name: sd-download
  #   volumes:
  #     - ./automatic/data:/data
  # automatic:
  #   <<: [ *gpu ]
  #   # build: https://github.com/AbdBarho/stable-diffusion-webui-docker.git#:services/AUTOMATIC1111
  #   build: https://github.com/Mithras/stable-diffusion-webui-docker.git#:services/AUTOMATIC1111 # hotfix
  #   container_name: automatic
  #   # restart: unless-stopped
  #   hostname: automatic
  #   environment:
  #     - CLI_ARGS=--allow-code --medvram --xformers --enable-insecure-extension-access --api --gradio-auth Mithras:${AUTOMATIC_PWD}
  #   ports:
  #     - 7860:7860
  #   volumes:
  #     - ./automatic/data:/data
  #     - ./automatic/output:/output
  # local-ai:
  #   # functionary-small-v3.2.Q8_0.gguf - text works, json doesn't work
  #   <<: [ *gpu ]
  #   image: localai/localai:latest-gpu-nvidia-cuda-12
  #   command:
  #     # text-, function~
  #     - huggingface://bartowski/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3-Q8_0.gguf
  #     # text+, function-
  #     # - huggingface://bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf
  #     # text+, function-
  #     # - huggingface://bartowski/Mistral-Nemo-Instruct-2407-GGUF/Mistral-Nemo-Instruct-2407-Q8_0.gguf
  #     # fails
  #     # - huggingface://TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/dolphin-2.7-mixtral-8x7b.Q4_K_M.gguf
  #     # text-, function~
  #     # - huggingface://TheBloke/laser-dolphin-mixtral-2x7b-dpo-GGUF/laser-dolphin-mixtral-2x7b-dpo.Q5_K_M.gguf
  #   container_name: local-ai
  #   # restart: unless-stopped
  #   hostname: local-ai
  #   volumes:
  #     - ./local-ai/data/models:/build/models
  #   ports:
  #     - 8080:8080
  #   networks:
  #     - homelab
networks:
  homelab:
    name: homelab
