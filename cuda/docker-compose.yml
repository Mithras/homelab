# sudo nvidia-ctk runtime configure --runtime=docker
# sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# sudo nvidia-ctk config --in-place --set nvidia-container-runtime.mode=cdi
# sudo systemctl restart docker
x-gpu-all: &gpu-all
  runtime: nvidia
  devices:
    - nvidia.com/gpu=all
x-gpu-5090: &gpu-5090
  runtime: nvidia
  devices:
    - nvidia.com/gpu=0
x-gpu-3090: &gpu-3090
  runtime: nvidia
  devices:
    - nvidia.com/gpu=1

services:
  # https://github.com/mostlygeek/llama-swap
  # http://localhost:8080
  llama-swap:
    <<: [ *gpu-all ]
    image: ghcr.io/mostlygeek/llama-swap:cuda
    container_name: llama-swap
    restart: unless-stopped
    hostname: llama-swap
    volumes:
      - ./llama-swap/config.yaml:/app/config.yaml
      - ./llama-swap/cache:/root/.cache/llama.cpp
    ports:
      - 8080:8080
    networks:
      - homelab

  # https://github.com/speaches-ai/speaches/
  # http://localhost:8001/
  # http://localhost:8001/v1/registry?task=text-to-speech
  # + curl http://localhost:8001/v1/models/speaches-ai/Kokoro-82M-v1.0-ONNX -X POST
  # http://localhost:8001/v1/registry?task=automatic-speech-recognition
  # + curl http://localhost:8001/v1/models/Systran/faster-whisper-large-v3 -X POST
  # + curl http://localhost:8001/v1/models/distil-whisper/distil-large-v3.5-ct2 -X POST
  speaches:
    <<: [ *gpu-3090 ]
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: speaches
    environment:
      UVICORN_PORT: "8001"
      # ENABLE_UI: "False"
      CHAT_COMPLETION_BASE_URL: http://llama-swap:8080/upstream/qwen3moe_instruct/v1
      TTL: "-1"
      USE_BATCHED_MODE: "True"
    restart: unless-stopped
    hostname: speaches
    volumes:
      - ./speaches/cache:/home/ubuntu/.cache/huggingface/hub
    ports:
      - 8001:8001
    networks:
      - homelab

  # https://github.com/musistudio/claude-code-router
  # http://localhost:3456/ui/
  claude-code-router:
    build: https://github.com/musistudio/claude-code-router.git
    ports:
      - "3456:3456"
    volumes:
      - ./claude-code-router:/root/.claude-code-router
    restart: unless-stopped
    networks:
      - homelab
    
networks:
  homelab:
    name: homelab

# ---

# https://github.com/teabranch/open-responses-server
# open-responses-server:
#   image: ghcr.io/teabranch/open-responses-server:latest
#   container_name: open-responses-server
#   restart: unless-stopped
#   hostname: open-responses-server
#   environment:
#     OPENAI_BASE_URL_INTERNAL: http://llama-swap:8080/upstream/qwen3moe_instruct
#     OPENAI_BASE_URL: http://0.0.0.0:8081
#     OPENAI_API_KEY: na
#     API_ADAPTER_PORT: 8081
#   volumes:
#     - ./open-responses-server/test.db:/app/src/open_responses_server/test.db
#     - ./open-responses-server/servers_config.json:/app/src/open_responses_server/servers_config.json
#   ports:
#     - 8081:8081
#   networks:
#     - homelab

# https://github.com/vllm-project/vllm
# vllm:
#   <<: [ *gpu-5090 ]
#   ipc: host
#   image: vllm/vllm-openai:latest
#   container_name: vllm
#   # restart: unless-stopped
#   hostname: vllm
#   command: >
#     --gpu-memory-utilization 0.9
#     --port 8001

#     --model Qwen/Qwen3-32B-AWQ
#     --max-model-len 8192
#     --enable-reasoning
#     --reasoning-parser deepseek_r1

#   # --model Qwen/Qwen3-32B-AWQ
#   # --model cognitivecomputations/Qwen3-30B-A3B-AWQ

#   # --trust-remote-code
  
#   # --enable-prefix-caching
#   # --enable-chunked-prefill

#   # --enable-auto-tool-choice
#   # --tool-call-parser hermes
#   environment:
#     VLLM_ATTENTION_BACKEND: FLASHINFER
#     # VLLM_USE_V1: "1"
#     # VLLM_MAX_SIZE_MB: "2048"
#     # VLLM_FLASH_ATTN_VERSION: "2"
#     # VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
#   volumes:
#     - ./vllm/huggingface/cache:/root/.cache/huggingface
#   ports:
#     - 8001:8001
#   networks:
#     - homelab
