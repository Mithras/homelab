# sudo nvidia-ctk runtime configure --runtime=docker
# sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# sudo nvidia-ctk config --in-place --set nvidia-container-runtime.mode=cdi
# sudo systemctl restart docker
x-gpu-all: &gpu-all
  runtime: nvidia
  devices:
    - nvidia.com/gpu=all
x-gpu-5090: &gpu-5090
  runtime: nvidia
  devices:
    - nvidia.com/gpu=0
x-gpu-3090: &gpu-3090
  runtime: nvidia
  devices:
    - nvidia.com/gpu=1

services:

  llama-swap:
    <<: [ *gpu-5090 ]
    image: ghcr.io/mostlygeek/llama-swap:cuda
    container_name: llama-swap
    restart: unless-stopped
    hostname: llama-swap
    volumes:
      - ./llama-swap/config.yaml:/app/config.yaml
      - ./llama-swap/cache:/root/.cache/llama.cpp
    ports:
      - 8080:8080
    networks:
      - homelab

  # https://github.com/teabranch/open-responses-server
  open-responses-server:
    image: ghcr.io/teabranch/open-responses-server:latest
    container_name: open-responses-server
    restart: unless-stopped
    hostname: open-responses-server
    environment:
      OPENAI_BASE_URL_INTERNAL: http://llama-swap:8080/upstream/qwen3moe_coder
      OPENAI_BASE_URL: http://0.0.0.0:8081
      OPENAI_API_KEY: na
      API_ADAPTER_PORT: 8081
    volumes:
      - ./open-responses-server/test.db:/app/src/open_responses_server/test.db
      - ./open-responses-server/servers_config.json:/app/src/open_responses_server/servers_config.json
    ports:
      - 8081:8081
    networks:
      - homelab

  # TODO: move to homelab
  # https://github.com/open-webui/mcpo
  # http://localhost:6274/
  # http://localhost:6274/?transport=sse&serverUrl=https://mithras.duckdns.org:8443/mcp_server/sse
  # http://localhost:6274/?transport=sse&serverUrl=http://mcp-proxy:8096/servers/everything/sse
  mcp-inspector:
    image: ghcr.io/modelcontextprotocol/inspector
    container_name: mcp-inspector
    restart: unless-stopped
    hostname: mcp-inspector
    environment:
      HOST: 0.0.0.0
      DANGEROUSLY_OMIT_AUTH: "true"
    ports:
      - 6274:6274 # ui
      - 6277:6277 # api
    networks:
      - homelab

  # TODO: move to homelab
  # https://github.com/sparfenyuk/mcp-proxy?tab=readme-ov-file#about
  # http://localhost:8096/status
  mcp-proxy:
    build:
      context: ./mcp-proxy
    container_name: mcp-proxy
    restart: unless-stopped
    hostname: mcp-proxy
    command: >
      --transport sse
      --port 8096
      --host 0.0.0.0
      --named-server-config /app/config.json
      --pass-environment
    # --debug
    environment:
      MEMORY_FILE_PATH: /app/memory/memory.json
      BASIC_MEMORY_HOME: /app/basic-memory
    volumes:
      - ./mcp-proxy/config.json:/app/config.json
      - ./mcp-proxy/filesystem:/app/filesystem
      - ./mcp-proxy/memory:/app/memory
      - /home/mithras/Obsidian/Main:/app/obsidian:ro
      - ./mcp-proxy/basic-memory:/app/basic-memory
    ports:
      - 8096:8096
    networks:
      - homelab

  # TODO: move to homelab
  # https://github.com/open-webui/mcpo
  # http://localhost:8000/docs
  mcpo:
    image: ghcr.io/open-webui/mcpo:dev
    container_name: mcpo
    restart: unless-stopped
    hostname: mcpo
    command: >
      --config /app/config.json
    # --header '{"Authorization": "Bearer ${HA_TOKEN}"}'
    volumes:
      - ./mcpo/config.json:/app/config.json
    ports:
      - 8000:8000
    networks:
      - homelab

  # https://github.com/speaches-ai/speaches/
  # http://localhost:8001/v1/registry?task=text-to-speech
  # + curl http://localhost:8001/v1/models/speaches-ai/Kokoro-82M-v1.0-ONNX -X POST
  # http://localhost:8001/v1/registry?task=automatic-speech-recognition
  # + curl http://localhost:8001/v1/models/Systran/faster-whisper-large-v3 -X POST
  # + curl http://localhost:8001/v1/models/distil-whisper/distil-large-v3.5-ct2 -X POST
  speaches:
    <<: [ *gpu-3090 ]
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: speaches
    environment:
      UVICORN_PORT: "8001"
      # ENABLE_UI: "False"
      # CHAT_COMPLETION_BASE_URL: TODO
      TTL: "-1"
      USE_BATCHED_MODE: "True"
    restart: unless-stopped
    hostname: speaches
    volumes:
      - ./speaches/cache:/home/ubuntu/.cache/huggingface/hub
    ports:
      - 8001:8001
    networks:
      - homelab

  # https://github.com/roryeckel/wyoming_openai/
  wyoming-openai:
    image: ghcr.io/roryeckel/wyoming_openai:latest
    container_name: wyoming-openai
    environment:
      STT_OPENAI_URL: http://speaches:8001/v1
      STT_MODELS: Systran/faster-whisper-large-v3
      # STT_STREAMING_MODELS: Systran/faster-whisper-large-v3
      STT_BACKEND: SPEACHES
      TTS_OPENAI_URL: http://speaches:8001/v1
      TTS_MODELS: speaches-ai/Kokoro-82M-v1.0-ONNX
      TTS_BACKEND: SPEACHES
      # TTS_SPEED: "0.5"
    restart: unless-stopped
    hostname: wyoming-openai
    ports:
      - 10300:10300
    networks:
      - homelab

networks:
  homelab:
    name: homelab

# ---

# wyoming-whisper:
#   <<: [ *gpu-3090 ]
#   # https://github.com/linuxserver/docker-faster-whisper
#   image: lscr.io/linuxserver/faster-whisper:2.3.0-gpu
#   container_name: wyoming-whisper
#   environment:
#     - PUID=1000
#     - PGID=1000
#     - TZ=America/Los_Angeles
#     # - WHISPER_MODEL=distil-large-v3
#     - WHISPER_MODEL=large-v3-turbo
#     - WHISPER_BEAM=5
#     - WHISPER_LANG=en
#   restart: unless-stopped
#   hostname: wyoming-whisper
#   volumes:
#     - ./wyoming-whisper/data:/config
#   ports:
#     - 10301:10300
#   networks:
#     - homelab
# networks:
# homelab:
#   name: homelab
  
# ollama:
#   # ? ollama pull hf.co/unsloth/Qwen3-32B-GGUF:Q4_K_M
#   # ? ollama pull hf.co/unsloth/Qwen3-30B-A3B-GGUF:Q4_K_M
#   # ? ollama pull qwen3:30b
#   <<: [ *gpu-5090 ]
#   image: ollama/ollama
#   container_name: ollama
#   restart: unless-stopped
#   hostname: ollama
#   # --hidethinking
#   # --think
#   environment:
#     - OLLAMA_ORIGINS=*
#     - OLLAMA_KEEP_ALIVE=300
#     # - OLLAMA_SCHED_SPREAD=1
#   volumes:
#     - ./ollama/data:/root/.ollama
#   ports:
#     - 11434:11434
#   networks:
#     - homelab

# llama-swap:
#   <<: [ *gpu-5090 ]
#   image: ghcr.io/mostlygeek/llama-swap:cuda
#   container_name: llama-swap
#   restart: unless-stopped
#   hostname: llama-swap
#   volumes:
#     - ./llama-swap/config.yaml:/app/config.yaml
#     - ./llama-swap/cache:/root/.cache/llama.cpp
#   ports:
#     - 8080:8080
#   networks:
#     - homelab

# vllm:
#   <<: [ *gpu-5090 ]
#   ipc: host
#   image: vllm/vllm-openai:latest
#   container_name: vllm
#   # restart: unless-stopped
#   hostname: vllm
#   command: >
#     --gpu-memory-utilization 0.9
#     --port 8001

#     --model Qwen/Qwen3-32B-AWQ
#     --max-model-len 8192
#     --enable-reasoning
#     --reasoning-parser deepseek_r1

#   # --model Qwen/Qwen3-32B-AWQ
#   # --model cognitivecomputations/Qwen3-30B-A3B-AWQ

#   # --trust-remote-code
  
#   # --enable-prefix-caching
#   # --enable-chunked-prefill

#   # --enable-auto-tool-choice
#   # --tool-call-parser hermes
#   environment:
#     VLLM_ATTENTION_BACKEND: FLASHINFER
#     # VLLM_USE_V1: "1"
#     # VLLM_MAX_SIZE_MB: "2048"
#     # VLLM_FLASH_ATTN_VERSION: "2"
#     # VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
#   volumes:
#     - ./vllm/huggingface/cache:/root/.cache/huggingface
#   ports:
#     - 8001:8001
#   networks:
#     - homelab
